---
title: "Quiz4768_Fleckenstein"
author: "Rylie Fleckenstein"
date: "11/4/2020"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r}
x <- c(1,2)
y <- c(1,0)

plot(x,y, col="blue",
     main="Visualizing the SVM Decision Boundary",
     xlim=c(0,5),
     ylim=c(0,5))
points(2,3,pch=17, col = "red")

```

The task for this quiz is the following: Build the SVM best linear classifier for the following (tiny) data set shown in Figure above. Show your solutions for w and b. You get additional points by solving the margin Ï=2/|w|. Add your SVM decision boundary on the figure below.

We can see that there are 2 classes and 3 different points. Class one has two points $p(1,1)$ and $p(2,0)$ and class two has one point $p(2,3)$. Since this data is linearly separable we can solve it through a geometric approach. Because of the nature of this data set we know the maximum margin weight vector is parallel to the line that connects the two closest points of opposite classes. So, to determine which two points are the closest (which point from class 1 is closest to the one point from class two) we compute their distances with the distance formula.

$$d = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} $$
From the above formula we calculate that $d(p(1,1), p(2,3)) = \sqrt5$ and $d(p(0,2), p(2,3)) = 2\sqrt2$ therefore, the two closest points are $p(1,1)$ and $p(2,3)$.

The next piece of information that we need is the weight vector $\vec{w}$. This can be computed through the following formula $<x_2-x_1,y_2-y_1>$ meaning the weight vector is equal to the following 
$$\vec{w}=<1,2>$$

We know that the optimal decision surface is orthogonal to the line created between the two closest points and also intersects it on its midpoint. Therefore the midpoint of that line is also a point that the decision surface passes through. We can calculate that using the following formula:
$$M = (\frac{x_1+x_2}{2}, \frac{y_1+y_2}{2} )$$
Therefore, $M = (1.5,2)$

Now, since we have the weight vector and a point of the decision surface we can now calculate the intercept using the following equation:
$$\vec{w}^T\vec{x} = -b $$
This equates to $$-b = \binom 12 \cdot(1.5,2)$$ 
so $b=-5.5$

Now we can rewrite the equation of the decision surface using the following format:

$$y = w_1x_1+w_2x_2+b$$

making the decision boundary equation : $y=x_1+2x_2-5.5$

The next important piece of information is that the constraint of the optimization is the following:
$$sign(y_i(\vec{w}^T\vec{x}_i+b))\geq1 $$

Also, an important note to make about the geometric margin in an SVM is that it is invariant to scaling parameters. Therefore, multiplying it by scalar properties will not effect it. With this information, we can conclude the following:
$\vec{w} = (a,2a)$ for any $a$ as well as the equation of the line is the following:
$$y = \vec{w}^Tx_i+b $$ from all of this we can determine the optimal hyperplane (the values of $\vec{w}$ and $b$)

so the calculations go as follows:

using the equation of the line and $y$ being equal to the class the data point belongs to (-1,1) and the $\vec{w}=(a,2a)$ and $\vec{x_i}=(x_i,y_i)$ coming from the two closest points that we found earlier, we get the following equations for the support vectors:

$$-1=\binom {a}{2a}\cdot(1,1)+b $$
and
$$1=\binom {a}{2a}\cdot(2,3)+b $$

this all equates out to 
$$a + 2a + b = -1 $$
and
$$ 2a + 6a + b = 1$$

From here we can use solve for a and b using a simple substitution method.

we conclude $a=\frac{-b-1}{3}$ and $b=-1-3a$ we then substitute these values back into the proper equations and return the values of 
$$a=2/5$$
and
$$b=-11/5$$

Finally, we can conclude the weight vector values and the intercept of the optimal hyperplane for this SVM:
$$\vec{w}=(2/5,4/5)$$
and
$$b=-11/5$$
we can also solve for the margin at this point using the equation:
$$\rho=\frac{2}{\lvert\vec{w}\rvert} $$

This equates out to
$$= \frac{2}{\sqrt{(2/5)^2+(4/5)^2}}$$
$$\rho=\sqrt5$$

Finally, we will draw the decision boundary on the graph of the data set:
```{r}
x <- c(1,2)
y <- c(1,0)

plot(x,y, col="blue",
     main="Visualizing the SVM Decision Boundary",
     xlim=c(0,5),
     ylim=c(0,5))
points(2,3,pch=17, col = "red")
abline(a=11/4,b=-.5)
abline(a=(11/4)+((sqrt(5))/2),b=-.5,lty='dotdash',col='red')
abline(a=(11/4)-((sqrt(5))/2),b=-.5,lty='dotdash',col='red')
```